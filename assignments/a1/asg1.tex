\documentclass[11pt,onecolumn]{article}
\usepackage{booktabs, multirow} % for borders and merged ranges
\usepackage{amsmath,amssymb,graphicx,algorithmic,xspace,url}
\usepackage[titlenotnumbered,noend,noline]{algorithm2e}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{color,soul}
\usepackage{array}
\usepackage{csquotes}
\usepackage{dirtytalk}



\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\setlength{\evensidemargin}{-0.1in} \setlength{\oddsidemargin}{-0.1in}
\setlength{\textwidth}{6.5in} \setlength{\textheight}{9.0in}
\setlength{\topmargin}{-0.6in}

\sloppypar
\begin{document}

\begin{center}
\begin{Large}
ECE 493, Spring 2020, Assignment 1\\
Due: Friday June 19, 11:59pm
\end{Large}
\end{center}
\vspace{.2em}

\vspace{.2em}
\begin{center}
Submit to the UWaterloo Crowdmark site using the link you received via email.
Your answer can be handwritten converted to an electronic file by and scanner or camera; or the answers can be typed up in a word processor or LaTeX and submitted as a pdf.
\end{center}
\vspace{.2em}

\vspace{.2em}


\section{Basics of Probability}

\begin{enumerate}
    \item Let $X$ and $Y$ be two random variables. If $X$ and $Y$ are uncorrelated, does it imply $X$ and $Y$ are independent? If yes, show how, else provide a counterexample. 
    \setlength{\parskip}{6pt}

    Definitions:
    \begin{itemize}
        \item $Cov(X,Y) = E[XY] - E[X]E[Y]$
        \item $Corr(X,Y) = \frac{Cov(X,Y)]}{\sqrt{Var(X)*Var(Y)}}$
    \end{itemize}
    To be uncorrelated means to have a $Covariance$ of 0:
    \begin{equation}
        E[XY] = E[X]E[Y].
    \end{equation}
    \hl{$\therefore$ $X$ and $Y$ are independent.} 
    
    \item A satellite is sending a binary message/code, i.e.\ a sequence of 1s and 0s. Suppose 70\% of the message to be sent is 0s. There is a 80\% chance of a given 0 or 1 being received correctly. Find the probability that 0 was sent if 1 was received. 
    \setlength{\parskip}{6pt}

    Definitions:
    \begin{itemize}
        \item $P(A \vert B) = \frac{P(B \vert A)*P(A)}{P(B \vert A)P(A) + P(B \vert A^c)P(A^c)}$
    \end{itemize}
    Let the event $transmission=0_2$ be represented as $t0_2$ and the event  $received = 1_2$ be represented as $r1_2$.
    \begin{equation}
        P(transmission=0_2 \vert received = 1_2) =  P(t0_2 \vert r1_2)
    \end{equation}
    
    \begin{equation}
        \begin{aligned}
        P(t0_2 \vert r1_2) & =  \frac{P(r1_2 \vert t0_2)P(t0_2)}{P(r1_2 \vert t0_2)P(t0_2) + P(r1_2 \vert t1_2)P(t1_2)} \\ 
        & = \frac{(1-0.8)*0.7}{(1-0.8)*0.7 + 0.8*0.3} \\
        & = 0.37
        \end{aligned}
    \end{equation}
    \hl{$\therefore$ The probability that a $0_2$ was sent given that a $1_2$ was received is $37\%$.}
    \item \textbf{Basic Inference}: The probability of getting a headache is $1/10$, getting the flu is $1/40$, and if you have a flu, there's $1/2$ chance of getting the headache. One day you wake up with a headache and say `Dang, I have a headache. Since 50\% of flues are associated with headaches, I must have a 50-50 chance of coming down with the flu'. Is this line of reasoning correct? Why/Why not?
    \setlength{\parskip}{6pt}

    Definitions:
    \begin{itemize}
        \item $P(A \vert B) = \frac{P(B \vert A)*P(A)}{P(B \vert A)P(A) + P(B \vert A^c)P(A^c)}$
    \end{itemize}

    This line of reasoning is not correct because the problem states that the of getting the a headache given you have the flu is 50\%. The problem \textbf{does not} state that the probability of getting the flu given you have a headache is 50\%. These two statements are not equivalent.

    Let $h$ be the event that you have a headache, and $f$ be the event that you have the flu. The problem describes the following conditional probability:
    \begin{equation}
        \begin{aligned}
        P(f \vert h) & =  \frac{P(h \vert f)*P(f)}{P(h \vert f)P(f) + P(h \vert f^c)P(f^c)} \\
        & =  \frac{0.5*0.025}{0.5*0.025 + 0.1*(1-0.025)} \\ 
        & =  0.11 \\ 
    \end{aligned}
    \end{equation}
    \hl{$\therefore$ There is only an 11\% chance of getting the flu given you have a headache, not 50\%.}

    \item \textbf{Bayesian Learning}: We are interested in forming a belief (hypothesis) about our environment based on what we have observed (evidence). Bayes rule is important when we want to assign a probability to our hypothesis after observing some evidence. Recall Bayes Rule:
    
    \begin{equation*}
        \mathbb{P}(H|e) = \frac{\mathbb{P}(e|H) \mathbb{P}(H)}{\mathbb{P}(e)},
    \end{equation*}
    where $H$ is the hypothesis, $e$ is the evidence, $\mathbb{P}(H)$ is our prior distribution over the different hypotheses, $\mathbb{P}(e|H)$ is the likelihood of the evidence given our hypothesis, $\mathbb{P}(H|e)$ is the posterior distribution of the different hypotheses given the evidence, and $\mathbb{P}(e)$ is the probability of the evidence.
    
    Suppose there are five kinds of bags of marbles: 
    \begin{enumerate}
        \item 10\% are bags of type $h_1$ with 100\% red marbles.
        \item 20\% are bags of type $h_2$ with 75\% red marbles and 25\% green marbles.
        \item 40\% are bags of type $h_3$ with 50\% red marbles and 50\% green marbles.
        \item 20\% are bags of type $h_4$ with 25\% red marbles and 75\% green marbles.
        \item 10\% are bags of type $h_5$ with 100\% green marbles.
    \end{enumerate}
    
    We buy a bag at random, pick 5 marbles from the bag and observe them to be all green. We are interested in knowing what type of bag we bought and what colour the next marble will be. 
    
    Let $e$ be the event that the 5 marbles drawn are green and let $\mathbb{P}(e) = \alpha$. 
    
    \hl{ASSUMPTION: Each marble pick is independent (picking marbles then putting thme back in the bag).}

    \begin{enumerate}
        \item Given that the 5 marbles drawn were green, compute the probability of each type of bag i.e.\ what is the probability of bag type $h_i$ for $i = \{1, 2, 3, 4, 5\}$, given that 5 marbles picked were green?
        \setlength{\parskip}{6pt}
            
        Definitions:
        \begin{itemize}
            \item $P(A \vert B) = \frac{P(B \vert A)*P(A)}{P(B \vert A)P(A) + P(B \vert A^c)P(A^c)}$
        \end{itemize}
        \begin{equation}
            \begin{aligned}
                P(h_i \vert 5g) &= \frac{P(5g \vert h_i)*P(h_i)}{P(5g \vert h_i)*P(h_i) + P(5g \vert h_i^c)P(h_i^c)} \\ 
            \end{aligned}
        \end{equation}
        \begin{equation}
            \begin{aligned}
                P(h_1 \vert 5g) &= \frac{0*0.1}{0*0.1 + 0.25^5*0.2 +  0.5^5*0.4 + 0.75^5*0.2 + 1^5*0.1} \\
                & = 0
            \end{aligned}
        \end{equation}
        \begin{equation}
            \begin{aligned}
                P(h_2 \vert 5g) &= \frac{0.25^5*0.2}{0*0.1 + 0.25^5*0.2 +  0.5^5*0.4 + 0.75^5*0.2 + 1^5*0.1} \\
                & = 0.00122
            \end{aligned}
        \end{equation}

        \begin{equation}
            \begin{aligned}
                P(h_3 \vert 5g) &= \frac{0.5^5*0.4}{0*0.1 + 0.25^5*0.2 +  0.5^5*0.4 + 0.75^5*0.2 + 1^5*0.1} \\
                & = 0.078
            \end{aligned}
        \end{equation}

        \begin{equation}
            \begin{aligned}
                P(h_4 \vert 5g) &= \frac{0.75^5*0.2}{0*0.1 + 0.25^5*0.2 +  0.5^5*0.4 + 0.75^5*0.2 + 1^5*0.1} \\
                & = 0.30
            \end{aligned}
        \end{equation}

        \begin{equation}
            \begin{aligned}
                P(h_5 \vert 5g) &= \frac{1^5*0.1}{0*0.1 + 0.25^5*0.2 +  0.5^5*0.4 + 0.75^5*0.2 + 1^5*0.1} \\
                & = 0.62
            \end{aligned}
        \end{equation}
        
        \item Use the results from part (a) to find the value of $\alpha$ using the axioms of probability.
        \setlength{\parskip}{6pt}

        Definitions:
        \begin{itemize}
            \item $P(A) = P(A \vert B)*P(B) + P(A \vert B^c)*P(B^c)$
        \end{itemize}
        
        \begin{equation}
            \begin{aligned}
                P(e) & = \sum_{i=1}^{i=5}P(e \vert h_i)*P(h_i) \\ 
                    & = 0*0.1 + 0.25^5*0.2 +  0.5^5*0.4 + 0.75^5*0.2 + 1^5*0.1
                    & = 0.16
            \end{aligned}
        \end{equation}
        \hl{$\therefore \alpha = 0.16$}
        \item Let $X$ be a random variable that represents the outcome of the next pick from the bag i.e.\ $X$ can be either green or red. How would you make a prediction about the outcome of $X$, given that the first 5 marbles picked were green? (Give an equation for $\mathbb{P}(X|e)$) Hint: Think about how you can use the concept of joint and marginal distributions with the events of picking different types of bags.
        \setlength{\parskip}{6pt}

        Let $Y$ represent a random variable for the number of green marbles drawn from the bag. Then to solve for 
        \begin{equation}
            p_{X\vert Y}(x \vert e) = \frac{p_{YX}(e,x)}{p_Y(e)} 
        \end{equation}

        \item Use the result from part (c) to compute $\mathbb{P}(X=\text{"green"} |e)$.
        \setlength{\parskip}{6pt}

        Let the event of the 6th marble picked being green be represented as $g$.
        \begin{equation}
            \begin{aligned}
                p_{X\vert Y}(g \vert e) & = \frac{p_{XY}(g,e)}{p_Y(e)} \\
                p_{X\vert Y}(g \vert e) & = \frac{p_{Y \vert X}(e,g)*p_X(g)}{p_Y(e)} \\
            \end{aligned}
        \end{equation}
        From 4.b, it can be seen that:
        \begin{equation}
            \begin{aligned}
                p_{Y}(e) & = 0.16. 
            \end{aligned}
        \end{equation}
        Following a similar process as 4.a \& 4.b, it can be seen that:
        \begin{equation}
            \begin{aligned}
                p_{X}(g) & = 0^1*0.1 + 0.25^1*0.2 +  0.5^1*0.4 + 0.75^1*0.2 + 1^1*0.1 \\
                & = 0.5
            \end{aligned}
        \end{equation}
        Due to the assumption stated above, of marble pickings being independent:
        \begin{equation}
            \begin{aligned}
                p_{XY}(g,e) & = p_X(g)*p_Y(e) \\
                & =  0.5*0.16 \\
                & = 0.08
            \end{aligned}
        \end{equation}
    \end{enumerate}
    Putting it all together:
    \begin{equation}
        \begin{aligned}
            p_{X\vert Y}(g \vert e) & = \frac{p_{Y \vert X}(e,g)*p_X(g)}{p_Y(e)} \\
            & = \frac{0.5*0.16*0.5}{0.16} \\
            & = 0.25
        \end{aligned}
    \end{equation}
    \hl{$\therefore$ The probability of picking a green marble after picking 5 green marbles is 25\%.}
\end{enumerate} 


\section{Multi-Armed Bandits}
\begin{enumerate}
    \item Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using $\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is $A_1$ = 1, $R_1$ = 1, $A_2$ = 2, $R_2$ = 1, $A_3$ = 2, $R_3$ = 2, $A_4$ = 2, $R_4$ = 2, $A_5$ = 3, $R_5$ = 0. On some of these time steps the $\epsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?
    \setlength{\parskip}{6pt}
    
    \begin{table}[!htp]\centering
        \caption{k-Armed Bandit Actions}\label{tab:kab1}
        \scriptsize
        \begin{tabular}{ccccll}\toprule
        Step & Action & Reward & Action with Largest Reward& $\epsilon$ action  & Greedy action\\\midrule
        1   &1 &1  &  &Maybe  &Maybe \\
        2   &2 &1  &1  &Yes    &No \\
        3   &2 &2  &1,2  &Maybe  &Maybe \\
        4   &2 &2  &2  &Maybe  &Maybe \\
        5   &3 &0  &2  &Yes  &No \\
        \end{tabular}
    \end{table}
    Note: All actions can potentially be an $\epsilon$ action, as an $\epsilon$ action choice is independent of action values (meaning it can pick the best action), \say{
        ...instead select randomly from among all the actions with equal probability, independently of the action-value estimates
    }.

    \item Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 10 and 20 with probability 0.5 (case A), and 90 and 80 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expected reward you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expected reward you can achieve in this task, and how should you behave to achieve it?
    \setlength{\parskip}{6pt}
    
    For the first part of the question, it does not matter which initial bandit is chosen. After choosing an initial bandit, the greedy approach should be employed (exploiting and never exploring). If the greedy approach is taken, 50\% of the choices will lead to un-optimal cases, and 50\% of the choices will lead to optimal cases, but the reward difference between optimal and sub-optimal is the same for action 1 and action 2. 
    \setlength{\parskip}{6pt}
    
    For example, if we iterate on  $2x$ number of actions, we get the following rewards depending on the initial bandit chosen.
    \begin{itemize}
        \item A1 initial bandit: x*10 + x*90 = x*100
        \item A2 initial bandit: x*20 + x*80 = x*100
    \end{itemize}

    For the second part of the question, if we know whether the environment is in state $A$ or state $B$, the first set of actions should be spent exploring the environment to completely map out a single case, either $A$ or $B$. This will take at most 3 actions. Once a single case has been mapped, the agent can choose the greedy action for that state, and the opposite action for the second case. A drawback to this approach is that it is not a scalable solution to problems with more than 2 bandits.
    \setlength{\parskip}{6pt}
    
    For example, if we find that action 1 is optimal in case $A$, for all case $A$ environments the agent will choose action 1, and action 2 for case $B$.

\end{enumerate}



\section{MDPs}

\begin{enumerate}
    \item Let $G_t = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{T-1} R_{T}$. Suppose $\gamma = 0.5$ and the following sequence of rewards is received: $R_1$ = 1, $R_2$ = 2, $R_3$ = 5, $R_4$ = -2, $R_5$ = 2, and $T$ = 5. Solve for $G_0$, $G_1$, $G_2$, $G_3$, $G_4$, and $G_5$.
    \setlength{\parskip}{6pt}
    
    Definitions:
    \begin{itemize}
        \item $G_t = R_{t+1} + \gamma G_{t+1}$
        \item $G_T = 0$
    \end{itemize}
    \begin{equation}
        \begin{aligned}
            G_5 = 0
        \end{aligned}
    \end{equation}

    \begin{equation}
        \begin{aligned}
            G_4 & = R_{5} + \gamma G_{5} \\ 
            & = 2 + 0.5 * 0 \\
            & = 2
        \end{aligned}
    \end{equation}

    \begin{equation}
        \begin{aligned}
            G_3 & = R_{4} + \gamma G_{4} \\ 
            & = -2 + 0.5 * 2 \\
            & = -1
        \end{aligned}
    \end{equation}

    \begin{equation}
        \begin{aligned}
            G_2 & = R_{3} + \gamma G_{3} \\ 
            & = 5 + 0.5 * -1 \\
            & = 4.5
        \end{aligned}
    \end{equation}

    \begin{equation}
        \begin{aligned}
            G_1 & = R_{2} + \gamma G_{2} \\ 
            & = 2 + 0.5 * 4.5 \\
            & = 4.25
        \end{aligned}
    \end{equation}

    \begin{equation}
        \begin{aligned}
            G_0 & = R_{1} + \gamma G_{1} \\ 
            & = 1 + 0.5 * 4.25 \\
            & = 3.125
        \end{aligned}
    \end{equation}
    \item Let $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$. Suppose $\gamma = 0.9$ and the reward sequence is $R_1 =2$, followed by an infinite sequence of 6s. What are $G_0$ and $G_1$?
    \setlength{\parskip}{6pt}
    
    Definitions:
    \begin{itemize}
        \item $\sum_{k=0}^{\infty} ar^k = \frac{a}{1-r}$ for $\vert r \vert < 1$
    \end{itemize}
    \begin{equation}
        \begin{aligned}
        G_1 & = \sum_{k=0}^\infty 0.9^k6\\
        & = \frac{6}{1-0.9}\\
        & = 60\\ 
        \end{aligned}
    \end{equation}

    \begin{equation}
        \begin{aligned}
        G_0 & = R_{1} + \gamma G_{1} \\ 
        & = 2 + 0.9*60\\
        & = 56\\ 
        \end{aligned}
    \end{equation}
    \item Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task? (For explanation of pole-balancing see Example 3.4 on Sutton's RL Book)
    \setlength{\parskip}{6pt}

    The return at each instance will be $-\gamma ^{T-t-1}$. In contrast the continuous case has a reward at each time instance of $-\gamma^{K}$ where $K$ represents the number of time steps until failure from the current state. After receiving a failure, $K$ refers to the next failure for the continuous case. Comparing these two cases, it can be seen that the continuous case can be represented as a infinite sequence of this episodic task, where $T=K$ in each subsequence.
    
    \item Figure \ref{fig:book3.14} (left) shows a rectangular grid world representation of a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Actions that would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions result in a reward of 0, except those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to A'. From state B, all actions yield a reward of +5 and take the agent to B'. 
    
    Assuming discount factor $\gamma=0.9$ and equal probabilities to all four actions, value functions seen in Figure \ref{fig:book3.14} (right) obtained. Show numerically that the Bellman expectation equation holds for the cell at the middle of top row, valued at +4.4, with respect to its four neighboring states, valued at +8.8, +2.3, +5.3 and off-grid (These numbers are accurate only to one decimal place.) (\textbf{Hint:} Try to understand what happens when agent takes action to north, off-grid)
    
\end{enumerate}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/mdp_grid.png}
    
    \caption{Gridworld example: exceptional reward dynamics (left) and state-value function for the equiprobable random policy (right)}
    \label{fig:book3.14}
\end{figure}
\setlength{\parskip}{6pt}

Definitions:
\begin{itemize}
    \item $\upsilon_{\pi}(s) = \sum_{a} \pi(a \vert s) \sum_{s \prime, r}(s\prime, r \vert s, a)[r + \gamma\upsilon_{\pi}(s\prime)]$
\end{itemize}
Let's break the bellman expectation equation into it's components.
\begin{equation}
    \begin{aligned}
        \upsilon_{\pi}(s) & = \sum_{a} \pi(a \vert s) \sum_{s \prime, r}(s\prime, r \vert s, a)[r + \gamma\upsilon_{\pi}(s\prime)] \\
        & = \frac{0+0.9*8.8}{4}+\frac{0+0.9*5.3}{4} + \frac{0+0.9*2.3}{4} + \frac{-1+0.9*4.4}{4} \\ 
        & = 4.43 \\ 
        & = 4.4
    \end{aligned}
\end{equation}
\hl{$\therefore$ the Bellman expectation equation holds for the cell at the middle of the top row.}
\end{document}
shown in the figure), along with a reward, r, depending on its dynamics given by the