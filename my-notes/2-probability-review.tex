\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{biblatex}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{marginnote}
\usepackage{hyperref}


\addbibresource{bib.bib}
\setlength{\parindent}{0em}
\bibliography{bib}


\title{Probability review}
\author{Zahin Mohammad}
\date{May 2020}
\linespread{1.5}
\begin{document}

\maketitle

\section{Elements of probability}

    $A \cap B$ can be read as $A$ and $B$ (the common area in a ven diagram).
    $A \cap B = 0$ signifies that there are no common areas between $A$ and $B$, they are disjoint (mutually exclusive).    
    \setlength{\parskip}{6pt}

    Independence is defined as $P(A \cap B) = P(A)P(B)$ or $P(A|B) = P(A)$. Therefore disjoint events are not independent.
    \setlength{\parskip}{6pt}

    Sample space $\Omega$ is the set of all outcomes in the experiment.
    \setlength{\parskip}{6pt}

    Event space $F$ is a subset of $\Omega$ of possible values of the experiment.
    \setlength{\parskip}{6pt}

    Probability measure $P$ is a function that takes event(s) and returns their probability of being true. If all events are disjoint, the probability of the events is the summation of the probability of individual events.
    \setlength{\parskip}{6pt}

    Properties:
    \begin{itemize}
        \item $A \subseteq B \Rightarrow P(A) \leq P(B)$
        \item $P(A \cap B) \leq min(P(A), P(B))$
        \item $P(A \cup B) \leq P(A) + P(B)$
        \item $P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}$ This is conditional probability
        \item $P(s_1 \cap s_2 ... \cap s_n) = P(s_1)P(s_2|s_1)P(s_3|s_1 \cap s_2)...P(s_n|s_{n-1} \cap s_{n-2}... \cap s_1)$ This is chain rule
    \end{itemize}

\section{Random variables}

    Discrete random variable: The random variable can only take finite values. If $X$ is a random variable denoting 10 coin tosses, it is discrete. $Val(x)$ denotes all possible events of random variable $X$. $Val(Die Throw) = {1,2,...6}$.
    \setlength{\parskip}{6pt}

    Continuous random variable: The random variable can take on infinite values. Cannot find probability of a single point, but instead a range. If $X$ is a random variable denoting the height of a randomly selected tree (can keep dividing the height of the last unit into smaller units).

    \subsection{CDF: Cumulative distribution function}
    Basically the sum of all probabilities up until an event. Usually used for continuous random variables.
    \begin{equation}
        F_X(x) = P(X \leq x)
    \end{equation}

    \subsection{PMF: Probability Mass function}
    Probability of a single event. Can only be used for discrete random variables.
    \begin{equation}
        p_X(x) = P(X=x)
    \end{equation}

    \subsection{PDF: Probability density functions}
    Defined only if CDF is differentiable everywhere. PDF is just the derivative of the CDF.
    \begin{equation}
        f_X(x)=\frac{dF_X(x)}{dx}
    \end{equation}

    Properties:
    \begin{itemize}
        \item $f_X(x) \geq 0$
        \item $\int_{-\infty}^{\infty} f_X(x) dx = 1$ Intuitively this makes sense because $f_X(x) \approx p_X(x)$
    \end{itemize}

    \subsection{Expectation}
    Expectation is a weighted average of a random variable. Let $g(X)$ be random variable, that takes input $X$ another random variable.
    \setlength{\parskip}{6pt}

    Discrete:
    \begin{equation}
        E[g(X)] = \sum_{x \in Val(X)}^{} g(x)p_X(x)
    \end{equation}
    Continuous:
    \begin{equation}
        E[g(X)] = \int_{- \infty}^{\infty} g(x)f_X(x)dx
    \end{equation}
    \setlength{\parskip}{6pt}

    Note: $E[X] = \int_{- \infty}^{\infty} xf_X(x)dx$ 

    Properties:
    \begin{itemize}
        \item $E[a] = a$
        \item $E[aX] = aE[X]$
        \item $E[X + Y] = E[X] + E[Y]$
    \end{itemize}

    \subsection{Variance}
    Variance is how concentrated a distribution is around the mean. 
    \begin{align}
        Var[X] & = E[X-E[X]]^2 \\
        & = E[X^2] - (E[X])^2
    \end{align}

    Properties:
    \begin{itemize}
        \item $Var[a] = 0$
        \item $Var[aX] = a^2Var[X]$
    \end{itemize}

    \subsection{Common random variables}
    Discrete:
    \begin{itemize}
        \item Bernouli
        \item Binomial
        \item Geometric
        \item Poisson
    \end{itemize}
    Continuous:
    \begin{itemize}
        \item Uniform
        \item Exponential
        \item Normal
    \end{itemize}

    Check the \href{https://rateldajer.github.io/ECE493T25S19/preliminaries/probabilityreview/}{course notes} for their properties.

\section{Two random variables}
    An example is $X$ representing number of heads in a coin flip and $Y$ representing the longest streak of heads.
    \subsection{Joint and marginal distributions}
    \begin{equation}
        F_{XY}(x,y) = P(X<x, Y<y)
    \end{equation}
    The marginal cumulative distribution functions are defined as
    \begin{align}
        F_Y(y) & = \lim{x \to \infty}F_{XY}(x,y) \\ 
        F_X(x) & = \lim{y \to \infty}F_{XY}(x,y).
    \end{align}

    \subsection{Joint and marginal probability mass functions}
    \marginnote{Forming the marginal distribution with respect to one variable by summing out the other variable is often known as “marginalization.”}
    \begin{equation}
        p_{XY}(x,y) = P(X=x, Y=y)        
    \end{equation}
    The marginal probability mass functions are defined as 
    
    \begin{align}
        p_{X}(x) & = \sum_y P_{XY}(x,y) \\   
        p_{Y}(y) & = \sum_x P_{XY}(x,y)     
    \end{align}

    \subsection{Joint and marginal probability density functions}
    Let $X$ and $Y$ be continuous random variables and their joint distribution function is differentiable in $x$ and $y$.

    \begin{equation}
        f_{XY}(x,y)=\frac{\partial^2 F_{XY}(x,y)}{\partial x \partial y}
    \end{equation}

    \begin{equation}
        \int \int_{(x,y) \in A} {f_{XY}(x,y)dxdy = P((X,Y) \in A)}
    \end{equation}
    Note: The integral over infinity is 1, even though the joint probability density function is always non-negative and potentially above 1 (think dirac delta).

    \begin{align}
        f_X(x) & = \int_{-infty}^{\infty}f_{XY}(x,y)dy \\
        f_Y(y) & = \int_{-infty}^{\infty}f_{XY}(x,y)dx
    \end{align}
        
    \subsection{Conditional distributions}
    What is the probability distribution function of $Y$ when $X=x$.
    \setlength{\parskip}{6pt}

    Discrete Case:
    \begin{equation}
        p_{Y|X}(y|x) = \frac{p_{XY}(x,y)}{p_X(x)}
    \end{equation}
    \setlength{\parskip}{6pt}
    
    Continuous Case:
    \begin{equation}
        f_{Y|X}(y|x)=\frac{f_{XY}(x,y)}{f_X(x)}
    \end{equation} 

    \subsection{Chain Rule}
    \begin{equation}
        p_{X_1,...X_n}(x_1,...x_n) \\ 
        = p_{X_1}(x_1)p_{X_2|x_1}(x_2 | x_1)...p_{X_n|X_1...X_n-1}(x_n|x_1,...x_n)
    \end{equation}

    \subsection{Bayes Rule}
    Discrete:
    \begin{align}
        P_{Y|X}(y|x) & = \frac{P_{XY}(x,y)}{P_X(x)} \\
        & = \frac{P_{X|Y}(X|Y)P_Y(y)}{\sum_{y\prime \in Val(Y)}P_{X|Y}(x|y\prime)P_Y(y\prime)}
    \end{align}

    Continuous:
    \begin{align}
        f_{Y \mid X}(y\mid x) & = \frac{f_{XY}(x, y)}{f_X(x)} \\ 
        & = \frac{f_{X \mid Y} (x \mid y) f_Y(y)}{\int^{\infty}_{- \infty} f_{X\mid Y} (x \mid y') f_Y (y') dy'}.
    \end{align}

    \subsection{Independence}
    \begin{itemize}
        \item $F_{XY}(x,y) = F_X(x)F_Y(y)$
        \item Discrete: $p_{XY}(x,y) = p_X(x)p_Y(y)$
        \item Discrete: $p_{Y|X}(y|x) = p_Y(y)$
        \item Continuous: $f_{XY}(x,y) = f_X(x)p_Y(y)$
        \item Continuous: $f_{Y|X}(y|x) = f_Y(y)$
    \end{itemize}

    \subsection{Expectation and covariance}
    \begin{itemize}
        \item $E[g(X,Y)] = \sum_{x \in Val(X)} \sum_{y \in Val(Y)} g(x, y)p_{XY}(x, y)$
        \item $E[g(X, Y)] = \int^{\infty}_{-\infty} \int^{\infty}_{-\infty} g(x, y)f_{XY}(x, y)dxdy$
        \item $Cov[X,Y] = E[(E-E[X])(Y-E[Y])] = E[XY]-E[X]E[Y]$
    \end{itemize}
    Note: $cov[X.Y] = 0$ is called uncorrelated.

    Properties:
    \begin{itemize}
        \item $Var[X+Y] = Var[X]+Var[Y]+2Cov[X, Y]$
        \item If $X$ and $Y$ are independent then $Cov[X,Y]=0$
        \item If $X$ and $Y$ are independent then $E[f(X)g(Y)] = E[f(X)]E[g(Y)]$
    \end{itemize}

\printbibliography[title={Referências}]
\end{document}
